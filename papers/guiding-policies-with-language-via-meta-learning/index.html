<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Paper a Week  | Guiding Policies with Language via Meta-Learning</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.59.1" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.d98f2eb6bcd1eaedb7edf166bd16af26.css" rel="stylesheet">
    

    

    
      
    

    
    
    <meta property="og:title" content="Guiding Policies with Language via Meta-Learning" />
<meta property="og:description" content="Guiding Policies with Language via Meta-Learning Problem:
 Reward functions and imitation learning have disadvantages for policy learning Reward functions require manual engineering while demonstrations require a human expert to be able to perform the task  Solution:
 Natural language instructions can overcome those disadvantages by being able to specify in detail the goals of the agent However, one instruction would not be enough so it is proposed that iterative language corrections are provided to an agent." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://example.org/papers/guiding-policies-with-language-via-meta-learning/" />
<meta property="article:published_time" content="2019-10-13T08:47:11+01:00" />
<meta property="article:modified_time" content="2019-10-13T08:47:11+01:00" />
<meta itemprop="name" content="Guiding Policies with Language via Meta-Learning">
<meta itemprop="description" content="Guiding Policies with Language via Meta-Learning Problem:
 Reward functions and imitation learning have disadvantages for policy learning Reward functions require manual engineering while demonstrations require a human expert to be able to perform the task  Solution:
 Natural language instructions can overcome those disadvantages by being able to specify in detail the goals of the agent However, one instruction would not be enough so it is proposed that iterative language corrections are provided to an agent.">


<meta itemprop="datePublished" content="2019-10-13T08:47:11&#43;01:00" />
<meta itemprop="dateModified" content="2019-10-13T08:47:11&#43;01:00" />
<meta itemprop="wordCount" content="296">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Guiding Policies with Language via Meta-Learning"/>
<meta name="twitter:description" content="Guiding Policies with Language via Meta-Learning Problem:
 Reward functions and imitation learning have disadvantages for policy learning Reward functions require manual engineering while demonstrations require a human expert to be able to perform the task  Solution:
 Natural language instructions can overcome those disadvantages by being able to specify in detail the goals of the agent However, one instruction would not be enough so it is proposed that iterative language corrections are provided to an agent."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="http://example.org/" class="f3 fw2 hover-white no-underline white-90 dib">
      Paper a Week
    </a>
    <div class="flex-l items-center">
      

      
      











    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        PAPERS
      </p>
      <h1 class="f1 athelas mb1">Guiding Policies with Language via Meta-Learning</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2019-10-13T08:47:11&#43;01:00">October 13, 2019</time>
      
      
    </header>

    <section class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><h1 id="guiding-policies-with-language-via-metalearning">Guiding Policies with Language via Meta-Learning</h1>

<p><strong>Problem</strong>:</p>

<ul>
<li>Reward functions and imitation learning have disadvantages for policy learning</li>
<li>Reward functions require manual engineering while demonstrations require a human expert to be able to perform the task</li>
</ul>

<p><strong>Solution:</strong></p>

<ul>
<li>Natural language instructions can overcome those disadvantages by being able to specify in detail the goals of the agent</li>
<li>However, one instruction would not be enough so it is proposed that iterative language corrections are provided to an agent. These corrections gradually guide the agent towards the final desired goal.</li>
<li>These corrections can be far more natural to provide than abstract reward functions or demonstrations</li>
</ul>

<p><strong>Algorithm</strong></p>

<ul>
<li>End-to-end algorithm for grounding iterative language corrections by using a multi-task setup to meta-train a model that can incorporate its previous behaviour and a correction so as to correct its policy to produce better actions

<ul>
<li>Agent attempts a task</li>
<li>Agent provided with a language correction after each attempt. This correction is language phrase according to some unknown stochastic function of the trajectory</li>
<li>Correction indicates how to improve the current trajectory to bring it closer to accomplishing the goal</li>
</ul></li>
</ul>

<p><strong>Model:</strong></p>

<p><figure><img src="/papers/attachments/e33ef43b.png" alt="Screenshot 2019-02-26 at 23.13.35.png"></figure></p>

<ul>
<li>Instructions are provided as a sequence of words which are converted into a sequence of word embeddings.</li>
<li>Each previous trajectory is processed by a 1D CNN to generated a trajectory embedding. The correction $C_i$, similar to the language description is converted into a sequence of word-embeddings.</li>
<li>Policy module has to integrate the high level description embedded into $Z_{im}$, the actionable changes from the correction module $Z_{cm}$ and environment state <em>s</em> to generated the right action</li>
<li>By iteratively incorporating language corrections, the model is in effect learning how to learn better analogous to meta-reinforcement learning recurrent models</li>
</ul>

<p><strong>Meta-training the GPL model</strong></p>

<ul>
<li>Meta-training tasks which will teach the model how to draw corrections from the task pool</li>
</ul>
<ul class="pa0">
  
</ul>
<div class="mt6">
      
      
      </div>
    </section>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <html>
<body>
<footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://example.org/" >
    &copy; 2019 Paper a Week
  </a>
    <div>










</div>
  </div>
</footer>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
