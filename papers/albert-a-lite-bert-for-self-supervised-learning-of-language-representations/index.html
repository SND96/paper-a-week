<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Paper a Week  | ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.59.1" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="https://snd96.github.io/paper-a-week/dist/css/app.d98f2eb6bcd1eaedb7edf166bd16af26.css" rel="stylesheet">
    

    

    
      
    

    
    
    <meta property="og:title" content="ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations" />
<meta property="og:description" content="ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations https://arxiv.org/pdf/1909.11942.pdf
Introduction  Increasing the model size when pretraining language represetnations often results in improvement on performance of downstream tasks. However, this comes at the cost of extra computing resources in terms of both memory required and speed. ALBERT introduces two parameter reduction techniques to improve performance of BERT without siginificant negative performance on downstream tasks  Factorizes embedding parametrization Cross-Layer parameter sharingg  ALBERT also introduces a self-supervised loss for sentence-order prediction which replaces the next sentence prediction task in BERT." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://snd96.github.io/paper-a-week/papers/albert-a-lite-bert-for-self-supervised-learning-of-language-representations/" />
<meta property="article:published_time" content="2020-06-07T08:47:11+01:00" />
<meta property="article:modified_time" content="2020-06-07T08:47:11+01:00" />
<meta itemprop="name" content="ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations">
<meta itemprop="description" content="ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations https://arxiv.org/pdf/1909.11942.pdf
Introduction  Increasing the model size when pretraining language represetnations often results in improvement on performance of downstream tasks. However, this comes at the cost of extra computing resources in terms of both memory required and speed. ALBERT introduces two parameter reduction techniques to improve performance of BERT without siginificant negative performance on downstream tasks  Factorizes embedding parametrization Cross-Layer parameter sharingg  ALBERT also introduces a self-supervised loss for sentence-order prediction which replaces the next sentence prediction task in BERT.">


<meta itemprop="datePublished" content="2020-06-07T08:47:11&#43;01:00" />
<meta itemprop="dateModified" content="2020-06-07T08:47:11&#43;01:00" />
<meta itemprop="wordCount" content="596">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations"/>
<meta name="twitter:description" content="ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations https://arxiv.org/pdf/1909.11942.pdf
Introduction  Increasing the model size when pretraining language represetnations often results in improvement on performance of downstream tasks. However, this comes at the cost of extra computing resources in terms of both memory required and speed. ALBERT introduces two parameter reduction techniques to improve performance of BERT without siginificant negative performance on downstream tasks  Factorizes embedding parametrization Cross-Layer parameter sharingg  ALBERT also introduces a self-supervised loss for sentence-order prediction which replaces the next sentence prediction task in BERT."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://snd96.github.io/paper-a-week/" class="f3 fw2 hover-white no-underline white-90 dib">
      Paper a Week
    </a>
    <div class="flex-l items-center">
      

      
      











    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        PAPERS
      </p>
      <h1 class="f1 athelas mb1">ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2020-06-07T08:47:11&#43;01:00">June 7, 2020</time>
      
      
    </header>

    <section class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><h1 id="albert-a-lite-bert-for-selfsupervised-learning-of-language-representations">ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations</h1>

<p><a href="https://arxiv.org/pdf/1909.11942.pdf">https://arxiv.org/pdf/1909.11942.pdf</a></p>

<h2 id="introduction">Introduction</h2>

<ul>
<li>Increasing the model size when pretraining language represetnations often results in improvement on performance of downstream tasks. However, this comes at the cost of extra computing resources in terms of both memory required and speed.</li>
<li>ALBERT introduces two parameter reduction techniques to improve performance of BERT without siginificant negative performance on downstream tasks

<ul>
<li>Factorizes embedding parametrization</li>
<li>Cross-Layer parameter sharingg</li>
</ul></li>
<li>ALBERT also introduces a self-supervised loss for sentence-order prediction which replaces the next sentence prediction task in BERT.</li>
</ul>

<h2 id="related-work">Related Work</h2>

<ul>
<li>Previous methods have improved efficiency in either memory or speed but adversely affect the other aspect.</li>
<li>Cross-layer parameter sharing has been used before but the prior work has been focused on training for standard encoder-decoder tasks rather than the pre-training/finetuning setting.</li>
<li>Previous work has removed the next-sentence prediction(NSP) task for BERT when experiments showed that had no effect on performance.</li>
</ul>

<h2 id="model-architecture">Model Architecture</h2>

<ul>
<li>The architecture is similar to BERT with transformer encoder with GELU nonlinearities.</li>
<li><span  class="math">\(E\)</span> is vocabulary embedding size, <span  class="math">\(L\)</span> is number of encoder layers, <span  class="math">\(H\)</span> is hidden size.</li>
</ul>

<h3 id="1-factorized-embedding-parameterization">1. Factorized Embedding Parameterization:</h3>

<ul>
<li>In BERT, the WordPiece embedding size E is tied with the hidden layer size <span  class="math">\(H\)</span> implying <span  class="math">\(E\equiv H\)</span></li>
<li>The authors argue that this is sub-optimal because these embeddings are meant to learn context-indpenedent representations whereas hidden-layer embeddings are meant to learn context-dependent representations. From previous papers, from the use of context, BERT-like representations help provide the signal for learning such context-dependent representations. Unlinking the two parametrs would make more efficient usage of the total model parameters by making it <span  class="math">\(H\gg E\)</span></li>
<li>Since <span  class="math">\(E\)</span> was originally very tied <span  class="math">\(H\)</span>, the total number of parameters of the embedding matrix <span  class="math">\(E\times V\)</span> would exponentially increase by increasing <span  class="math">\(H\)</span>. The resulting matrix would be sparse would only a few values being updated during training.</li>
<li>The embedding paramers size <span  class="math">\(O(V\times H)\)</span> grows with increasing <span  class="math">\(H\)</span>. ALBERT factorizes the embedding parameters into two smaller matrices with size <span  class="math">\(O(V\times E\ +\ E\times H)\)</span>. The vectors are projected to space <span  class="math">\(E\)</span> first before being projected to space <span  class="math">\(H\)</span>. This is much more significant when <span  class="math">\(H\gg E\)</span>.</li>
</ul>

<h3 id="2-crosslayer-parameter-sharing">2. Cross-Layer Parameter Sharing:</h3>

<ul>
<li>In order to improve parameter efficiency, ALBERT shared all parameters across layers. Compared to Deep Equilibrium Models which reach an equilibrium point for which the input and output embeddings of a certain llayer remain the same, ALBERT has oscillating embeddings.</li>
</ul>

<h3 id="3-intersentence-coherence-loss">3. Inter-Sentence Coherence Loss</h3>

<ul>
<li>BERT used two pre-training tasks, masked language modelling(MLM) and next sentence prediction(NSP).</li>
<li>NSP was designed to improve performance on downstream tasks that involve sentence pairs. However, later experiments and research showed that the task had minimal impact on performance.</li>
<li>The reason probably beingg that NSP was much simpler than MLM and as such was not a useful task in conjuction with MLM.</li>
<li>The authors argue that NSP conflates topic prediction and coherence prediction. While topic prediction is easier, it also overlaps with MLM.</li>
<li>ALBERT uses sentence-order prediction which classifies whether two sentences are in the correct order or not. This new classification loss is explicitly for sentence coherence.</li>
</ul>

<h2 id="evaluations">Evaluations</h2>

<ul>
<li>The paper has much more detailed explanations of experiments but as a summary, ALBERT improves performance of BERT on several different tasks while also being about 1.7 times faster (ALBERT-large vs BERT-large). ALBERT-xxlarge gains the most improvements in performance with only around 70% of BERT-large's parameters. However, the structure of this model makes it 3 times slower.</li>
</ul>

<p><figure><img src="https://snd96.github.io/paper-a-week/attachments/34241099.png" alt="Screenshot 2020-06-08 at 12.35.08.png"></figure></p>

<ul>
<li>Paper also details ablation studies and performance on GLUE tasks.</li>
<li>Overall, ALBERT-xxlarge has less parameters than BERT-large but gets significantly better results while losing speed due to its large structure.</li>
</ul>
<ul class="pa0">
  
</ul>
<div class="mt6">
      
      
      </div>
    </section>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <html>
<body>
<footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://snd96.github.io/paper-a-week/" >
    &copy; 2020 Paper a Week
  </a>
    <div>










</div>
  </div>
</footer>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


    

  <script src="https://snd96.github.io/paper-a-week/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
