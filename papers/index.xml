<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Papers on Paper a Week</title>
    <link>https://snd96.github.io/paper-a-week/papers/</link>
    <description>Recent content in Papers on Paper a Week</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 07 Jun 2020 08:47:11 +0100</lastBuildDate>
    
	<atom:link href="https://snd96.github.io/paper-a-week/papers/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations</title>
      <link>https://snd96.github.io/paper-a-week/papers/albert-a-lite-bert-for-self-supervised-learning-of-language-representations/</link>
      <pubDate>Sun, 07 Jun 2020 08:47:11 +0100</pubDate>
      
      <guid>https://snd96.github.io/paper-a-week/papers/albert-a-lite-bert-for-self-supervised-learning-of-language-representations/</guid>
      <description>Model based Reinforcement Learning for Atari https://arxiv.org/pdf/1903.00374.pdf
Introduction
 Model-free reinforcement learning has been used to learn effective policies for complex gaems such as Atari games just from the image observations. However, this requires a huge amount of iterations which is not similar to a human. This is possibly due to the fact that humans generate a model of the environment itself so that they can predict what is coming next and learn further from that.</description>
    </item>
    
    <item>
      <title>Model based Reinforcement Learning for Atari</title>
      <link>https://snd96.github.io/paper-a-week/papers/model-based-reinforcement-learning-for-atari/</link>
      <pubDate>Sun, 26 Apr 2020 08:47:11 +0100</pubDate>
      
      <guid>https://snd96.github.io/paper-a-week/papers/model-based-reinforcement-learning-for-atari/</guid>
      <description>Model based Reinforcement Learning for Atari https://arxiv.org/pdf/1903.00374.pdf
Introduction
 Model-free reinforcement learning has been used to learn effective policies for complex gaems such as Atari games just from the image observations. However, this requires a huge amount of iterations which is not similar to a human. This is possibly due to the fact that humans generate a model of the environment itself so that they can predict what is coming next and learn further from that.</description>
    </item>
    
    <item>
      <title>Deep Learning without Weight Transport</title>
      <link>https://snd96.github.io/paper-a-week/papers/deep-learning-without-weight-transport/</link>
      <pubDate>Sun, 19 Apr 2020 08:47:11 +0100</pubDate>
      
      <guid>https://snd96.github.io/paper-a-week/papers/deep-learning-without-weight-transport/</guid>
      <description>Deep Learning without Weight Transport https://arxiv.org/pdf/1904.05391.pdf
 Current algorithms for deep learning rely on weight transport, where the weight matrices used during the forward pass are transmitted backwards during the backpropagation algorithm. Biologically this is likely impossible as neurons in the brain don&#39;t transfer they weights between each other. The paper proposes 2 methods which let the feedback path learn appropriate weights accurately and quickly without weigt transport.  Weight-transport problem:</description>
    </item>
    
    <item>
      <title>Agent 57: Outperforming the Atari Human Benchmark</title>
      <link>https://snd96.github.io/paper-a-week/papers/agent-57-outperforming-the-atari-human-benchmark/</link>
      <pubDate>Sun, 12 Apr 2020 08:47:11 +0100</pubDate>
      
      <guid>https://snd96.github.io/paper-a-week/papers/agent-57-outperforming-the-atari-human-benchmark/</guid>
      <description>Agent 57 https://arxiv.org/pdf/2003.13350.pdf
Problem:
 Current state of the art reinforcement learning models for Atari games can only do well on a subset of the the 57 games available. Models that solve hard exploration problems like Pitfall can reach human level performance but can&#39;t do so for every game  Solution
 Agent57 incorporates many existing approaches to solve these hard exploration problems but iterates on them to achieve human level performance on all of the games and not just a subset The two problems that the model has to solve is,  Long-term credit assignment: Games where the reward is only assigned after a very long sequence of actions which means the agent has to infer how its current action will affect states that are many steps into the future.</description>
    </item>
    
    <item>
      <title>ViLBERT</title>
      <link>https://snd96.github.io/paper-a-week/papers/vilbert/</link>
      <pubDate>Fri, 21 Feb 2020 08:47:11 +0100</pubDate>
      
      <guid>https://snd96.github.io/paper-a-week/papers/vilbert/</guid>
      <description>ViLBERT Problem
 Currently the dominant strategy for vision-and-language tasks is to pretrain models for the two domains separately and then attempt learn visual grounding by training them together However, this approach does not accurately capture the relations between visual and linguistic features and how they effect one another. Need a model that is pre-trained for visual grounding.  Solution:
 Paper presents a joint model for learning task-agnostic visual grounding from paired visiolinguistic data which is called Vision &amp;amp; Language BERT.</description>
    </item>
    
    <item>
      <title>Experience Replay for Continual Learning</title>
      <link>https://snd96.github.io/paper-a-week/papers/experience-replay-for-continual-learning/</link>
      <pubDate>Sun, 24 Nov 2019 08:47:11 +0100</pubDate>
      
      <guid>https://snd96.github.io/paper-a-week/papers/experience-replay-for-continual-learning/</guid>
      <description>Experience Replay for Continual Learning https://arxiv.org/pdf/1811.11682.pdf
Problem
 In settings where gathering new experience is expensive/difficult it is imperative to retain gained knowledge while learning only from one task at a time. Boundaries between different tasks will not be persistent and this leads to the danger of catastrophic forgetting where an agent forgets what it has learned previously when it encounteres a new situtation  Solution
 An ideal continual learning system must satisfy three requirements.</description>
    </item>
    
    <item>
      <title>Finding Friend or Foe in Multi-Agent Games</title>
      <link>https://snd96.github.io/paper-a-week/papers/finding-friend-or-foe-in-multi-agent-games/</link>
      <pubDate>Sun, 10 Nov 2019 08:47:11 +0100</pubDate>
      
      <guid>https://snd96.github.io/paper-a-week/papers/finding-friend-or-foe-in-multi-agent-games/</guid>
      <description>Finding Friend or Foe in Multi-Agent Games https://arxiv.org/pdf/1906.02330.pdf
Problem
 There have been multiple breakthroughs in multi-agent games in recent research, however none of these games deal with the scenario where co-operation is with unknown team mates is uncertain. In games like Dota and Capture the Flag there is no ambiguity on who to co-operate with. In hidden role games like The Resistance: Avalon, discovering the roles of each player is a challenging task with similarities to deception games like poker.</description>
    </item>
    
    <item>
      <title>Revisiting Self-Supervised Visual Representation Learning</title>
      <link>https://snd96.github.io/paper-a-week/papers/revisiting-self-supervised-visual-representation-learning/</link>
      <pubDate>Sun, 03 Nov 2019 08:47:11 +0100</pubDate>
      
      <guid>https://snd96.github.io/paper-a-week/papers/revisiting-self-supervised-visual-representation-learning/</guid>
      <description>Revisiting Self-Supervised Visual Representation Learning https://arxiv.org/pdf/1901.09005.pdf
Problem:
Paper deals with aspects of self-supervised learning that have been not researched thouroughly or recent enough.
Self Supervision:
 Framework for creating supervised signal automatically in order to learn representations that will be useful for downstream tasks Requires only unlabled data in order to formulate a pretext learning task such as predicting context Pretext tasks must be designed in such a way that high level understanding is useful for solving them  Architecture of CNN models:</description>
    </item>
    
    <item>
      <title>Learning Unsupervised Learning Rules</title>
      <link>https://snd96.github.io/paper-a-week/papers/learning-unsupervised-learning/</link>
      <pubDate>Sun, 27 Oct 2019 08:47:11 +0100</pubDate>
      
      <guid>https://snd96.github.io/paper-a-week/papers/learning-unsupervised-learning/</guid>
      <description>Learning Unsupervised Learning Rules https://arxiv.org/pdf/1804.00222.pdf
Problem
 Unsupervised learning has yet to fulfil the potential of being powerful in situations with limited labelled data. Ideally learned representations should integrate high level attributes of data in the representations generated. However, possibly due to utilising incorrect target tasks during training, useful representations are only produced as a side effect.  Solution
 Use a semi-supervised task (meta objective) to help build improve the quality of representations using the unsupervised model.</description>
    </item>
    
    <item>
      <title>GAN Dissection</title>
      <link>https://snd96.github.io/paper-a-week/papers/gan-dissection/</link>
      <pubDate>Sat, 26 Oct 2019 08:47:11 +0100</pubDate>
      
      <guid>https://snd96.github.io/paper-a-week/papers/gan-dissection/</guid>
      <description>GAN Dissection https://arxiv.org/pdf/1811.10597.pdf
Problem  Interpretability of Generative Adversarial networks is still a mystery Example: To produce a church image, what does a GAN need to learn? How are these structures represented? What causes artifacts and why do different GAN variants work better?  Solution  Study the internal represenentations of GANs Understand how GAN represents structures and whether there is any way to interpret these structures for a human. A general method for visualizing and understanding GANs at different levels of abstraction is presented.</description>
    </item>
    
  </channel>
</rss>