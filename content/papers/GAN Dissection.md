---
katex: true
markup: "mmark"
date: 2019-10-26T08:47:11+01:00
title: "GAN Dissection"
description: "" 


draft: false
---

# GAN Dissection

## Problem

* Interpretability of Generative Adversarial networks is still a mystery
* Example: To produce a church image, what does a GAN need to learn? How are these structures represented?
* What causes artifacts and why do different GAN variants work better?

## Solution

* Study the internal represenentations of GANs
* Understand how GAN represents structures and whether there is any way to interpret these structures for a human.
* A general method for visualizing and understanding GANs at different levels of abstraction is presented.
  * Identify a group of interpretable units that are related to object concepts. These units' feature maps should closely match a semantic segmentation of the object in the image
  * With these units identified, try to manipulate their values and observe whether it is possible to make an object appear or disappear.
  * Examine the contextual relationship between these causal objects and the image background. Observe how manipulating these causal objects affects the image overall.
  * Insert these causal units related to an object concepts into new networks and observe the result.
  * Related to identifying object concepts, identify causal units which generate artifacts and attempt to rectify the problem
![Screenshot 2019-06-04 at 22.00.23.png](/attachments/0163a8e5.png)
 
 ## Dissection

 * Compare the upsampled heat map generated from the features with the semantic segmented object from the image.
 * If the agreement with the images is greater than a calculated threshold then the features are said to be related to the object
![Screenshot 2019-06-04 at 22.10.06.png](/attachments/525b4e98.png)

## Intervention

* With the feature units identified, start manipulating these units and observe the changes in the image generated
* The causal effects of forcing the units on and off is observed and measured.
* Causal effect is the average difference in pixels of the object being focussed on.
![Screenshot 2019-06-04 at 22.12.29.png](/attachments/576dcf6e.png)

## Method

### 1. Characterizing units by dissection 

* Dissection is measured by quantifying the spatial agreement between the unit *u*'s threshold feature map and a concept *c*'s segmentation with intersection-over-union measure.
* The threshold feature map is generated by using a binary mask where the pixel is active if the IoU measure surpasses the threshold.
* The threshold is selected by maximizing the information quality ratio **I/H** using a separate validation set. **I** is mutual information and **H** is entropy

### 2. Measuring causal relationships using intervention

* The network is probed to ascertain whether the set of units *u* cause the generation of concept *c* by forcing the units on and off
* An object is caused by units *u* if it appears when the units are on and disappears when the units are off

## Results summary

* The authors compare different GAN frameworks and also the layers in those frameworks. Graphs in the paper are self-explanatory
* The units causing artifacts are identified in an example which can then be decreased by ensuring the causal relationship between the units and the artifacts.
* The authors then add these units to different networks and observe the results. They remark that the success of the insertion of new units depends on the network and the type of image being generated. The context of where the units are inserted heavily matters 

